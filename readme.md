kepp the llama.cpp .gguf file running for server
run python backend/main.py
cd frontend
run npm run dev

connection perfect to llama.cpp
responds, mcp actives.
